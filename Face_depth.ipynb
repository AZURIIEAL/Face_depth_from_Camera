{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad340cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import cvzone\n",
    "from cvzone.FaceMeshModule import FaceMeshDetector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ddceeb",
   "metadata": {},
   "source": [
    "Creating a cv2 object to capture the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c545a3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap=cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dde3a3",
   "metadata": {},
   "source": [
    "Reading the camera to check basic input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7854b707",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m img\u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mflip(img,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#Adding the detector.\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m img,faces \u001b[38;5;241m=\u001b[39m\u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindFaceMesh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdraw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m faces:\n\u001b[0;32m     11\u001b[0m     face \u001b[38;5;241m=\u001b[39m faces[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GPU_Enabled\\lib\\site-packages\\cvzone\\FaceMeshModule.py:46\u001b[0m, in \u001b[0;36mFaceMeshDetector.findFaceMesh\u001b[1;34m(self, img, draw)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03mFinds face landmarks in BGR Image.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;124;03m:param img: Image to find the face landmarks in.\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m:param draw: Flag to draw the output on the image.\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03m:return: Image with or without drawings\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgRGB \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfaceMesh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimgRGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m faces \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults\u001b[38;5;241m.\u001b[39mmulti_face_landmarks:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GPU_Enabled\\lib\\site-packages\\mediapipe\\python\\solutions\\face_mesh.py:124\u001b[0m, in \u001b[0;36mFaceMesh.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the face landmarks on each detected face.\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;124;03m    face landmarks on each detected face.\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GPU_Enabled\\lib\\site-packages\\mediapipe\\python\\solution_base.py:334\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    328\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    330\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m    331\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    332\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 334\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[0;32m    337\u001b[0m solution_outputs \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mnamedtuple(\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSolutionOutputs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info\u001b[38;5;241m.\u001b[39mkeys())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Finding Face\n",
    "detector = FaceMeshDetector(maxFaces=1)\n",
    "while True:\n",
    "    success,img = cap.read()\n",
    "    #Needs to flip as using an external webcam\n",
    "    img= cv2.flip(img,1)\n",
    "    #Adding the detector.\n",
    "    img,faces =detector.findFaceMesh(img,draw=False)\n",
    "    \n",
    "    if faces:\n",
    "        face = faces[0]\n",
    "        #Marking a point under the left eye retina\n",
    "        pointLeft = face[145]\n",
    "        #Marking a point under the Right eye retina\n",
    "        pointRight = face[374]\n",
    "        #Making a circle to visualize that (BGR)\n",
    "        #Drawing\n",
    "        \n",
    "#         cv2.circle(img,pointLeft,5,(0,0,255),cv2.FILLED)\n",
    "#         cv2.circle(img,pointRight,5,(0,0,255),cv2.FILLED)\n",
    "#         cv2.line(img,pointLeft,pointRight,(255,0,0),3)\n",
    "        \n",
    "        w,_=detector.findDistance(pointLeft,pointRight)\n",
    "        \n",
    "        #Human values\n",
    "        #FInding the focal point\n",
    "        #\n",
    "        #d=50\n",
    "        #f=(w*d)/W\n",
    "        \n",
    "        #print(f)\n",
    "        \n",
    "        #finding the distance/depth\n",
    "        f=604\n",
    "        W=6.3\n",
    "        d=(W*f)/w\n",
    "        \n",
    "        text = f'Depth:{int(d)} cm'\n",
    "        coordinates = (50,50)\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        fontScale = 1\n",
    "        color = (0,0,255)\n",
    "        thickness = 2\n",
    "        img = cv2.putText(img, text, coordinates, font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "    \n",
    "    \n",
    "    cv2.imshow(\"Face_Depth_Analyzer\",img)\n",
    "    cv2.waitKey(1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9c2560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d670a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a34df36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
